{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "5b65d76b-5ef1-4d6a-8071-c1f5efc38362",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c175dcdc-b70e-4c82-8eb0-cb6cacccb8cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9c2be1-7c6f-4821-87ed-6ce9c0da0740",
   "metadata": {},
   "source": [
    "# *Text with abbveriations , incorrect words and unwanted digits*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "9dafe229-8eb7-40bf-83dd-346651cc0264",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_en=\"\"\"NLP is a field of7 coputer science and a  subfield of AI \n",
    " that aims to make computers understand human language. NLP uses computational linguistics, \n",
    " which is the stdy of how language works, and  various models based on staistics, machine learning,\n",
    " and deep learning.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c548e404-bbb3-4048-bba8-d0d31f380645",
   "metadata": {},
   "source": [
    "# *Function to remove noise*\n",
    " Remove non-alphanumeric characters ,\n",
    " Remove digits,\n",
    " Remove extra spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "e6091ef4-e9d2-4c85-addd-683fe0a4cef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_noise(text):\n",
    "    text = re.sub(r'\\W+', ' ', text)  \n",
    "    text = re.sub(r'\\d+', '', text)  \n",
    "    text = text.strip()  \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "70be3f67-7e47-4d20-be0b-c3b9325d2e3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLP is a field of coputer science and a subfield of AI that aims to make computers understand human language NLP uses computational linguistics which is the stdy of how language works and various models based on staistics machine learning and deep learning\n"
     ]
    }
   ],
   "source": [
    "text_en=remove_noise(text_en)\n",
    "print(text_en)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de88fddd-f5ed-4589-b48f-dbb4a348a0f8",
   "metadata": {},
   "source": [
    "# Function to Regex pattern for common contractions\n",
    "manual expandation the obbveriations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "1c1c06cd-bd94-4225-910b-c67c2806d6a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanded Text: nature language processing is a field of coputer science and a subfield of artificial intellegence that aims to make computers understand human language nature language processing uses computational linguistics which is the stdy of how language works and various models based on staistics machine learning and deep learning\n"
     ]
    }
   ],
   "source": [
    "def expand_abbreviations_regex(text):\n",
    "    text = re.sub(r\"\\bI'm\\b\", \"I am\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\"\\bAI\\b\", \"artificial intellegence\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\"\\bcan't\\b\", \"cannot\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\"\\bwon't\\b\", \"will not\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\"\\bn't\\b\", \" not\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\"\\bit's\\b\", \"it is\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\"\\bthat's\\b\", \"that is\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\"\\bASAP\\b\", \"as soon as possible\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\"\\bUK\\b\", \"United kindom\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\"\\bNLP\\b\", \"nature language processing\", text, flags=re.IGNORECASE)\n",
    "\n",
    "    return text\n",
    "\n",
    "text_en = expand_abbreviations_regex(text_en)\n",
    "print(\"Expanded Text:\", text_en)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00aef9c0-18a2-4618-afa4-33ea69cd6d6b",
   "metadata": {},
   "source": [
    "# Tokenizaion to remove Stopwords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "a9d7a6b4-bc14-47b7-8a0a-9016a25e319d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Tokens: ['nature', 'language', 'processing', 'field', 'coputer', 'science', 'subfield', 'artificial', 'intellegence', 'aims', 'make', 'computers', 'understand', 'human', 'language', 'nature', 'language', 'processing', 'uses', 'computational', 'linguistics', 'stdy', 'language', 'works', 'various', 'models', 'based', 'staistics', 'machine', 'learning', 'deep', 'learning']\n"
     ]
    }
   ],
   "source": [
    "def stop_words_removal(text):\n",
    "    # Tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Get English stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # Remove stopwords\n",
    "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "    \n",
    "    return filtered_tokens\n",
    "\n",
    "text_en = stop_words_removal(text_en)\n",
    "print(\"Filtered Tokens:\", text_en)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e952bb0c-4068-4f0a-af8d-237bbd8ee284",
   "metadata": {},
   "source": [
    "# Auto correct function \n",
    "correct words with missing characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "798d3c2e-6d0c-4630-a389-3584e9a04055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrected Text: ['nature', 'language', 'processing', 'field', 'computer', 'science', 'subfield', 'artificial', 'intelligence', 'aims', 'make', 'computers', 'understand', 'human', 'language', 'nature', 'language', 'processing', 'uses', 'computational', 'linguistic', 'study', 'language', 'works', 'various', 'models', 'based', 'statistics', 'machine', 'learning', 'deep', 'learning']\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    # Correct the text using TextBlob\n",
    "   blob = TextBlob(str(text_en))\n",
    "   corrected_text = blob.correct()\n",
    "   return corrected_text\n",
    "    \n",
    "text_en=lemmatize_text(text_en)\n",
    "print(\"Corrected Text:\", text_en)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b2dcd8-240f-4e31-9e7d-e2e6088e7562",
   "metadata": {},
   "source": [
    "# All propressing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "b3ef8a1c-0695-4d15-95ed-57524d1533e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nature', 'language', 'processing', 'field', 'computer', 'science', 'subfield', 'artificial', 'intelligence', 'aims', 'make', 'computers', 'understand', 'human', 'language', 'nature', 'language', 'processing', 'uses', 'computational', 'linguistic', 'study', 'language', 'works', 'various', 'models', 'based', 'statistics', 'machine', 'learning', 'deep', 'learning']\n"
     ]
    }
   ],
   "source": [
    "def preprocess_text(text):\n",
    "    # Step 1: Normalization\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Step 2: Remove Noise\n",
    "    text = remove_noise(str(text))\n",
    "    \n",
    "    # Step 3: Abbreviation Expansion\n",
    "    text = expand_abbreviations_regex(text)\n",
    "    \n",
    "    # Step 4: Remove Stop Words\n",
    "    text = stop_words_removal(text)\n",
    "    \n",
    "    # Step 5: Lemmatization\n",
    "    text = lemmatize_text(text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "final_text=preprocess_text(text_en)\n",
    "print(final_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3861467-3884-45d7-9b88-9d518da9d5d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
